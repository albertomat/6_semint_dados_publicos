---
title: "Oficina Chicoteando a Máquina para Extrair Dados: como manipular grandes bases em computadores de pouca capacidade"
subtitle: "6º Seminário Internacional sobre Análise de Dados na Administração Pública" 
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Requisitos

* Conhecimento básico da Linguagem R
* Conhecimento básico da Linguagem SQL
* Download do [DB Explorer SQL Lite](https://sqlitebrowser.org/dl/);
* Descompactador de arquivos
* Download do [R Base](https://cran.r-project.org/bin/windows/base/) atualizado (4.0.2) - *tem se mostrado mais rápido*;

## Principais Bibliotecas

* vroom
* RSqlite
* tidyverse
* data.table

```{r ativando as bibliotecas, warning=FALSE}
#Criar um loop aqui
if(!require('vroom')){
  install.packages("vroom")
  library(vroom)
}

if(!require('RSQLite')){
  install.packages("RSQLite")
  library(RSQLite)
}

if(!require('tidyverse')){
  install.packages("tidyverse")
  library(tidyverse)
}

if(!require('data.table')){
  install.packages("data.table")
  library(data.table)
}
```


## Bases de dados que utilizamos

* [RFB](https://receita.economia.gov.br/orientacao/tributaria/cadastros/cadastro-nacional-de-pessoas-juridicas-cnpj/dados-publicos-cnpj)
  - [Dicionário de dados](http://200.152.38.155/CNPJ/LAYOUT_DADOS_ABERTOS_CNPJ.pdf)
  
* [SEPT - Secretaia Especial de Previdência e Trabalho](http://pdet.mte.gov.br/microdados-rais-e-caged) - **Novo Caged 2020**
  - [Dicionário de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx)
  
* [PGFN](http://dadosabertos.pgfn.gov.br/)

# Importante! Caso tenha atualizado o R para a versão 4, fique atento e siga esses passos:


* 1) Atualize o R Studio  

* 2) Atualize o RTools para a versão 4 - Rtools4  
  - Adicione a variável de ambiente no R para que o Rtools4 funcione: `writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")`  

* 3) Se em alguns pacotes apresentarem erro de depêndencia, tente usar o  `install.packages("nome do pacote", dependencies = TRUE)`

## Para quem só sabe usar martelo, **TODO PROBLEMA É UM PREGO** - *Abraham Maslow*
<div style="text-align: justify">
... nosso martelo é leve (SQL) e já temos um pouquinho de experiência com a linguagem **R** que aprendemos na escola...
--> Por isso vamos usar SQL + R

"Concordando conosco..." rsrs , em seu livro [Efficient R Programming](https://csgillespie.github.io/efficientR/) Colin Gillespie e Robin Lovelace falam sobre o uso de bancos de dados com o R no [capitulo 6](https://csgillespie.github.io/efficientR/data-carpentry.html) - *6.6 Working with databases*:

"Instead of loading all the data into RAM, as R does, databases query data from the hard-disk. This can allow a subset of a very large dataset to be defined and read into R quickly, without having to load it first."

Veja só o quanto pode ser interessante este uso:

Dizem que 80% do tempo do cientista de dados é destinado para ajustar as bases, mas além de missing values, outliers e outros problemas, temos o aumento do volume de dados  disponíveis. 
Alguns analistas que trabalham em R já devem ter lotado a memória com uma base mais robusta. #quemnunca?
E aí, até conseguir provisionar alguma instância, pedir mais memória, ou conseguir autorização para usar um servidor de maior capacidade... ufa! É possível usar recursos alternativos para progredir com o trabalho.

<div/>

# Agora sim chegou a hora .... Mãos na massa!

**Plano de ataque**

Para atingir os objetivos da Oficina, vamos percorrer uma jornada que nos levará a realizar uma consulta, produzindo um sumário de 3 bases de dados diferentes que não caberiam na memória. Esse é o desafio que enfrentaremos conjuntamente.
 
Vamos baixar dos dados, verificar alguns detalhes que podem aparecer e, principalmente, corrigir problemas. Depois disso, vamos criar um banco de dados para receber as bases. Faremos a ingestão destes dados no banco diretamente do data.frame. De posse de um banco populado com os dados necessários, realizaremos as consultas. 

Vamos buscar fazer isso de forma bem simples e didática.

## Fase 1 - Baixando os dados

<div style="text-align: justify">

A primeira situação que nos deparamos é baixar as bases, são diferentes fontes **(http, ftp)**, disponibilizadas de diferentes formas **(csv, zip, gz)**, além da quantidade que pode variar, 1 arquivo por período, 1 arquivo com todos os dados.

Disponibilizamos link alternativo de download das bases no início da Oficina, já que uma quantidade de conexões simultâneas pode ser interpretado como um comportamento anormal... E estamos em treinamento, certo? Tentamos retirar as variáveis inesperadas de nossa equação pedagógica.

### Baixando a **base da PGFN** -> 1 arquivo 

As base de dados da [PGFN](http://dadosabertos.pgfn.gov.br/) contém os dados dos devedores do FGTS, previdênciários e não previdênciários. São arquivos compactados pelo tema (FGTS, Previdenciário ...) contendo arquivos **csv** para cada estado, com competência mensal.

Será criada uma pasta para armazenar estes downloads para facilitar a posteior leitura:

```{r Download dados PGFN, eval=FALSE}
## Dados de dividas
#diretorio para amazenamento
dir.create("./divida")

#Divida FGTS 
download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_FGTS.zip", "./divida/Dados_abertos_FGTS.zip", method = "auto")

```

Foram baixados 13 Mb de dados compactados


### Baixando a **base do Novo Caged** --> multiplos arquivos

Os microdados do [CAGED](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/) são fornecidos ao público por um ftp aberto em que os dados são dispostos em pastas por mês e cada pasta contem arquivos em formato **.7z** :( de todo o ano calendário. Faremos a leitura de todo o diretório e o download dos arquivos em pasta específica do tema.

```{r Download dados CAGED, eval=FALSE}
#Dados do CAGED
#diretorio para armazenamento
dir.create("./caged")

#FTP de acesso para os microdados
#atencao o mês foi especificiado manualmente, até a confeção deste, os dados do mês de agosto não foram disponibilizados.
url = "ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/Julho/"
filenames = getURL(url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
filenames <- strsplit(filenames, "\r\n")
filenames = unlist(filenames)

filenames

#laço baixando todos os arquivos dentro da pasta 
for (filename in filenames) {
  download.file(paste(url, filename, sep = ""), paste(getwd(), "/caged/", filename,
                                                      sep = ""), method = "wininet", mode ="wb")
}
# MAC ou linux os dados devem ser baixados diretamente
#cuidado com o metodo ao baixar do FTP o metodo pode corromper o arquivo  
```

Foram baixados 182 Mb de dados compactados.

### Baixando a **base CNPJ da RF** (#sqn)

Conforme indicamos no início da oficina, a base de CNPJ foi disponibilizada em formato SQLite compactada para download. 

Respeitando o licenciamento da base, levamos a conhecimento:

A licença do código é [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e dos dados convertidos [Creative Commons Attribution ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Fonte original: Receita Federal do Brasil Responsável por tratar os dados: Álvaro Justen/[Brasil.IO](https://brasil.io/).

Das tabelas dispobilizada mantivemos somente a que continha todos os CNPJ´s, de qualquer forma caso tenha curiosidade de ver como foi feita a montagem deste banco em sqlite  teremos um chunk mais abaixo **que está inativo** - ` eval=FALSE` que realiza o processo de ingestão dos dados caso tenha mais um tempinho.

**Falar sobre o desafio de rodar a base e colocar nos comentários o tempo**

Aqui acreditamos que já deu um tempo para baixar o download que passamos no início da oficina, **descompacte o arquivo meu_primeiro_db.zip na pasta do projeto** para podermos usá-lo

Terminamos assim o download dos aquivos, baixamos diferentes bases de dados em tamanho e características. Agora, estes arquivos serão trabalhados para que seja possível obter algum resultado.

## Fase 2 - Lendo os dados

Com os dados baixados, começa a jornada para de lê-los e ajustá-los e vice-versa. 

### Lendo **PGFN** -> 1 arquivo

O arquivo com os dados da PGFN com as dívidas do FGTS será lido e passado para um data.frame, usaremos o pacote *vroom* 

```{r Leitura dados PGFN, eval= FALSE }
#primeira pegadinha, o VROOM pode ser muito rápido, porem se você tiver multiplos arquivos dentro do zip ele precisa de uma ajudinha
dados_fgts_com_vroom <- vroom("./divida/Dados_abertos_FGTS.zip") # aqui temos 1170 Observacoes 

dados_fgts_com_vroom<-dados_fgts_com_vroom[,-4] #RETIRANDO O NOME DA EMPRESA

head(dados_fgts_com_vroom)
nrow(dados_fgts_com_vroom)
```

Conforme apresentado, o arquivo compactado que estamos tratando possui um *.csv* para cada estado e a quantidade de **1170** observações não parece condizer com a quantidade de empresas inscritas em divida no FGTS. Além disso, os caracteres quebrados podem dificultar as manipulações dos dados. A base foi disponiblizada com caracteres latinos (ç,õ,é, ...), e precisaremos informar isso para o pacote para conseguir a visualização adequada. 
Outro detalhe: precisaremos criar uma função que irá ler todos os arquivos dentro do *.zip*, passar cada 1 para o *vroom* e, por fim, incluir em um data.frame.

```{r Leitura dados FGTS, eval= FALSE}
##Instrução para ler os caractereres corretamente:
locale_padrao_pt <- locale("pt", encoding = "latin1")


#Criando uma função para ler todos os dados dentro do arquivo para que o VROOM "entenda" de forma correta
ler_arquivos_dentro_zip <- function(file, ...) {
  nome_dos_arquivos <- unzip(file, list = TRUE)$Name
  vroom(purrr::map(nome_dos_arquivos, ~ unz(file, .x)), locale = locale_padrao_pt,   ...)
}

#Indicando para a função o caminho do arquivo para leitura de todos os dados
tic()
leitura_direta_vroom <- ler_arquivos_dentro_zip("./divida/Dados_abertos_FGTS.zip")
toc()
```

**TROCAR PARA DATA.TABLE**

Agora que cada arquivo foi passado ao *vroom* e incluído em um data.frame, contabilizamos mais de 440 mil observações, bem diferente do primeiro valor contabilizado, não é?

```{r Verificando o dados da base FGTS}
leitura_direta_vroom <- leitura_direta_vroom[,-4] #RETIRANDO O NOME DA EMPRESA
head(leitura_direta_vroom)
nrow(leitura_direta_vroom)
```

Nestes dados, o campo **CPF_CNPJ** está com a máscara, e para a realização das consultas iremos retirá-la.

**Spoiler** na base que irá fornecer algumas informações para nós o campo CNPJ não possui a máscara, por isso temos que retirá-la.

```{r Retirar máscara do CNPJ}
#funcao do pacote tm que remove pontuacao
removePunctuation(leitura_direta_vroom$CPF_CNPJ) -> leitura_direta_vroom$CPF_CNPJ
```


### Lendo **base do Novo Caged** --> múltiplos arquivos

Ao se trabalhar com os microdados do CAGED temos alguns percalços. Ao verificar o [dicionario de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx) os nomes das colunas foram escritos com acentuação e os arquivos estão compactados em formato *.7z*. Para lidar com a primeira situação, realizaremos o tratamento das colunas. Quanto ao formato, para o andamento desta oficina, é necessário extrair os arquivos na pasta `caged` do nosso projeto (vide aba Files) :(

```{r Leitura dados CAGED, eval = FALSE}
tic()
#criando uma lista com arquivos txt no diretório
caged_lista_arquivos <- fs::dir_ls("./caged/", glob = "*txt")
#realizando a leitura dos dados  forçando a coluna saldo movimentação com inteiro e demais colunas como caracater
caged_movimentacao <- vroom(caged_lista_arquivos, col_types = list(.default = "c", saldomovimentação = "i" ))
toc()
```

Verificando o nome das colunas :

```{r Verificando o nome das colunas caged}
names(caged_movimentacao)
```

Realizamos a leitura de mais de 16 milhões de linhas com 24 colunas (Nuh!) e a carga de mais dados pode começar a ficar difícil. Dando continuidade, trataremos os nomes das colunas retirando os espaços e acentuação. Assim, vamos conseguir trabalhar com estes dados.


```{r Tratando nomes das colunas CAGED, eval= FALSE}
#Tratando os nomes
caged_movimentacao %>%
  names() %>% #pega os nomes das colunas
  stri_trans_general("Latin-ASCII")  -> names(caged_movimentacao)  # substitui caracteres latin para asci
 
```

Com os nomes padronizados, estamos com mais uma base pronta :)

```{r Nomes das colunas do CAGED }
#Verificando o numero de linhas
nrow(caged_movimentacao)
#verificando a nomenclatura das colunas
names(caged_movimentacao)
```

#### Lendo a **base CNPJ da RF** (#sqn) 
## Atenção! Desafio extra!

Aqui temos um trabalho para você realizar **depois da oficina** que é fazer a carga dos dados de todas as empresas no banco SQLite em sua máquina. Recomendamos **NÃO REALIZÁ-LO** durante a oficina, pois pode durar mais de 50 minutos. 
Para fins de aprendizado, disponbilizamos o código de como realizar a ingestão de um grande arquivo sem ter que descompactá-lo e passar diretamente para um banco de dados em SQLite.

**As linhas de código abaixo estão desativadas**
```{r Criando o banco cnpj, eval= FALSE}
#Estas linhas de código estão destivadas
#Ative-o somente caso tenha tempo para aguardar o download da base e sua ingestão.
#todos os cnpjs 
dir.create("./cnpj")

#maior base de dados 2.5Gb o tempo pode variar bem confome o a conexão
download.file("https://data.brasil.io/dataset/socios-brasil/empresa.csv.gz", "./cnpj/empresa.csv.gz", method = "auto")



#criando conexao com o banco
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")

#esta funcao e necessaria para alterar o tratamento de entrada dos dados das colunas.
#e criada uma tabela de streamming que le o arquivo e repassar para o arquivo sqlite
tabela_temp_streaming_cnpj <- function() {
  streamable_table(
    function(file, ...) readr::read_csv(file,col_types = list(.default = "c"),  ...),
    function(x, path, omit_header)
      readr::write_tsv(x = x, path = path, omit_header = omit_header),
    "tsv")
}

tic()
#unark pega arquivo gz passa para uma tabela de streaming gera aquivos de 100000 linhas  e passa para o banco sqlite
unark("./cnpj/empresa.csv.gz", con, lines=100000, streamable_table = tabela_temp_streaming_cnpj() )
toc()

#lembrar sempre de se desconectar do banco
dbDisconnect(con)
```


# ... Nosssa! Com todas estas bases a memória pode encher. E agora, José (ou João)?

Os dados vão aparecendo, a necessidade de agregar mais variáveis ou pegar algum atributo de outra base e chega uma hora que não se tem mais memória para o R trabalhar ou carregar mais dados, somente a base de cnpj´s em um único arquivo compactado tem 2.4Gb ao descompactá-lo são mais de 8Gb que você deveria carregar na memória.

# SQLITE
PATRICIA  
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf

Chegamos no ápice da oficina, agora que foram visualizados dados que já estavam em um banco, criaremos um conforme as nossas necessidades.

## Criando o banco ou abrindo um existente

Para criar um novo arquivo que receberá o banco ou ler um existente a sintaxe é a mesma e o código se encarrega de entender se o arquivo já existe ou não.

```{r Conexao com o banco RSQLite}
#Pasando para uma variável as instruções de conexao e driver - atencao para o nome do arquivo
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")
```

Ao executar o chunk acima o R está conectado com este arquivo e usando a API do DBI pode passar instruções  para que o RSQLite processe, inclusive em SQL !

Como temos um banco populado, alem de verificar com o dbExplorer, podemos verificar pelo R. Como a conexão acima já foi realizada ela está ativa e você pode passar a instrução diretamente, porem, ao final de seu código lembre sempre de fechar a conexão com o banco.


```{r Verificando as tabelas do banco}
#listando as tabelas do banco
dbListTables(con)
```

Assim como fazemos no R podemos verificar quais campos esta tabela possui.

```{r Verificando os campos da tabela empresa}
dbListFields(con, "empresa")
```

Vamos olhar os dados desta tabela 

```{r Verificando os dados da tabela empresa}
primeira_query_empresa <-
  dbGetQuery(con, "select * from empresa limit 10") #LIMIT é importante !!!

primeira_query_empresa
```

Veja que abaixo do nome do campo o tipo dele é apresentado **<chr>**, o cuidado com o tipo de variável é extremamente importante.

E quantos registros temos nesta tabela

```{r Contando registros da tabela empresa}
tic()
contagem_registros_empresa <-
  dbGetQuery(con, "select count(*) from empresa")
toc()


contagem_registros_empresa
```
Para varrer toda a tabela e contar todos os registros foi necessário um tempo, Leve em consideração este tempo para o desenho e estratégia das consultas.

Rapidamente, fizemos uma pequena exploratória no banco, temos mais de **44 milhões* de registos com 32 colunas cada. E temos mais duas tabelas em memória que usaremos para interagir com a tabela *empresa* 

## Realizando carga dos data frames

Como conseguimos realizar a leitura dos arquivos da dívida do **FGTS** vamos realizar a ingestão dele no banco.

```{r Carga tabela divida_fgts}
tic()
dbWriteTable(con, #conexão com o banco
             "divida_fgts",#nome da tabela no banco
             leitura_direta_vroom) #nnome do data.frame
toc()
rm(leitura_direta_vroom)
gc()
```

Agora com duas tabelas, o cenário começa a ficar um pouco mais robusto, porem, sem carregar na memória, podemos inclusive excluir o data frame com os dados do fgts.

```{r Verificando novamente as tabelas}
# Verificando as tabelas
dbListTables(con)
```
 Consultando os campos da tabela
```{r Verificando os campos da tabela divida_fgts}
#Verificando os campos da tabela
dbListFields(con, "divida_fgts")
```

Consultando os primeiros registros

```{r Verificando os dados da tabela divida_fgts }
tic()
dbGetQuery(con, "select * from divida_fgts limit 10")
toc()
```
 
Falta uma tabela a `caged_movimentacao` inclusive ela tem uma boa quantidade de registros. Para fixar o conhecimento vamos realizar mais uma carga de dados. Aqui dependendo do seu computador a consulta pode demorar mais de 15 minutos!! 

```{r Carga tabela movimentacao_caged}
tic()
dbWriteTable(con,
             "movimentacao_caged",
             caged_movimentacao)
toc()
rm(caged_movimentacao)
gc()
```
O tempo de carga parece uma eternidade...

Agora temos um banco com 3 tabelas completas de nossa oficina. Caso precise, por exemplo, disponibilizar os dados para estudo os mesmo já estão agrupados em um único arquivo. Você pode compactá-lo e disponibilizar e passar adiante.

**Verificar no dbExplorer**

```{r Verificando todas as tabelas}
dbListTables(con)
```

## Trabalhando as consultas

As consultas acima em sql que foram realizadas equivalem a um `head()`, `nrow()` e `names()`, porem, precisamos filtrar os dados, trabalhar mais com eles.  
Como primeira consulta realizaremos um filtro com campos existentes nas tabelas.

Antes de trabalhar o plano de ataque vamos realizar um aquecimento, realizando uma consula com um filtro das empresas que estão nesta base no DF.

```{r Consulta base com filto na query divida_fgts}
tic()
dbGetQuery(con,
           "select * from divida_fgts where UF_UNIDADE_RESPONSAVEL='DF'")
toc()
```

No DF temos 6610 empresas com tratativas sobre o FGTS.


Para a tabela de movimentação do caged verificaremos quantos linhas de movimentação foram realizadas este ano para o estado Rio de Janeiro. Caso não tenha visto no dicionário de dados na aba *uf* os estados estão codificados e o valor correspondente ao *Rio de Janeiro é 33*

```{r Consulta base com filto na query movimentacao_caged}
tic()
dbGetQuery(con, "select * from movimentacao_caged where uf=33")
toc()
```

Mais de 1 milhão de registros de movimentações sobre os empregados, nesta consulta exigimos um pouco mais do computador, porem, carregar estes dados em memória.

Para a tabela empresa seremos um pouco mais restritivos nos filtros pois é a tabela mais volumosa

```{r Consulta base com filto na query empresa}
tic()
dbGetQuery(con, "select * from empresa where codigo_municipio=5819")
toc()
```

Em Campo dos Goytacazes foram retornadas 92572 empresas pela consulta.

### R + RSQLite

Até o momento o R foi usado somente para passar as consultas ao RSQLite. Como próximo passo realizaremos uma consulta passando uma variável do ambiente do R.

Duas tabelas contam com o **CNAE - Classificação Nacional de Atividade Econômica** em seus dados a **movimento_caged** com o nome do campo de *subclasse* e a tabela **empresas** com o nome do campo *cnae_fiscal*. Faremos uma consulta na tabela em dos dados do CAGED com uma variável que receberá os seguintes CNAES:

* 8230001 - Serviços de Organização de Feiras, Congressos, Exposições e Festas
* 8230002 - Casas de Festas e Eventos

```{r lista de cnaes selecionados em memoria}
selecao_cnae <- c('8230001','8230002')
```

Na tabela `movimento_caged` consultaremos qual a movimentação de trabalhadores no estado da Bahia = 29 dos segmentos de CNAE selecionados.

```{r Consulta base com filtro em variavel caged }
tic()
dbGetQuery(
  con,
  "select * from movimentacao_caged where uf= 29 and subclasse = ?",
  params = list(selecao_cnae)
)
toc()
```

Foram computador 833 registros com estes filtros.

Aumentando as possibillidades iremos realizar uma consulta que trará a quantidade de empresas ativas baseado no atributo UF da tabela *empresa*. Será realizada a contagem de quantas empresas estão ativas (situacao_cadastral = 2 ) nos estados do Acre - AC, Mato Grosso - MT e Ceará - CE .

```{r Consulta a base com sumario empresa}
tic()
dbGetQuery(
  con,
  "select uf, count(*) from empresa where uf in ('AC','MT','CE') and situacao_cadastral = 2 group by uf"
)
toc()
```

### Query com subquery

Temos a seguinte situação: Faremos um resumo por CNAEs selecionados em todas as bases de dados, porem duas de 3 bases de dados possuem o CNAE em seus atributos *empresa e movimentacao_caged*. A tabela *empresa* possui a relação de todas as empresas e cnaes, usaremos o cnpj para levar o cnae desta tabela para a *divida_fgts* usando o CNPJ como referência e armazenaremos o resultado em uma tabela paralela para não ter que varrer toda a tabela *empresa*

**CNAES: **

* 6209100 - Suporte Técnico, Manutenção e Outros Serviços em Tecnologia da Informação
* 8230001 - Serviços de Organização de Feiras, Congressos, Exposições e Festas
* 4754701 - Comércio Varejista de Móveis

Os resultados das consutas serão retornadas em variáveis do ambiente do R.

Vamos consultar a tabela *movimentacao_caged*

```{r Sumario por cnae}
tic()
resultado_movimentacao_caged <-
  dbGetQuery(
    con,
    "select subclasse,sum(saldomovimentacao) as saldo_movimentacao from movimentacao_caged
where subclasse in (6209100,8230001,4754701)
group by subclasse"
  )
toc()
resultado_movimentacao_caged
``` 

Agora faremos a cosulta na tabela *empresa* com os mesmos CNAES

```{r Sumario por cnae_empresa}
tic()
resultado_quantidade_empresa <-
  dbGetQuery(
    con,
    "select cnae_fiscal,count(*)as total_empresas from empresa
where cnae_fiscal in (6209100,8230001,4754701)
group by cnae_fiscal"
  )
toc()
resultado_quantidade_empresa
```

Para realizar a consulta por CNAE na tabela divida_fgts, criaremos uma tabela com a relação *cnpj vs cnae* que será usada como subconsulta.

```{r Lista de subconsulta}
tic()
dbSendQuery(
  con,
  "create table lista_cnae as SELECT cnpj, cnae_fiscal from empresa where cnae_fiscal in (6209100,8230001,4754701)"
)
toc()
```

Agora que temos uma relação de *cnpj vs cnae* podemos usá-la como subconsulta na tabela *divida_fgts*

```{r Sumario por CNAE divida_fgts, error=TRUE}
tic()
resultado_saldo_divida <-
  dbGetQuery(
    con,
    "select cnae_fiscal, round(sum(VALOR_CONSOLIDADO),2) as total_divida from divida_fgts
left join lista_cnae on divida_fgts.CPF_CNPJ = lista_cnae.cnpj
where CPF_CNPJ in (SELECT cnpj from lista_cnae)
GROUP by cnae_fiscal"
  )
toc()
resultado_saldo_divida
```

Com todos os resultados depositados em variável e para quem estava com saudade de código em R segue.

```{r Montando o resultado final}
tic()
left_join(resultado_quantidade_empresa, resultado_movimentacao_caged, by = c("cnae_fiscal"="subclasse")) %>%
  left_join(.,resultado_saldo_divida, by = c("cnae_fiscal"))
toc()
```



#Existe outra alternativa ?

OK, fizemos algumas querys, mas... "João não estou familiarizado com sql", sei usar R.

Existe outra alternativa a biblioteca `dbplyr` te ajudará a conversar com o banco de uma forma "mais R"

Vamos criar um vinculo com a tabela *divida_fgts*

Você usará o mesmo parâmetro de conexão `con` para dizer em "qual lingua" você vai conversar com este banco

```{r DBPlyr Criando vinculo com o banco}
#Passamos para uma variável uma funcao do dplyr tbl para conversar com a tabela divida_fgts no banco
divida_fgts_rsqlite <- tbl(con,"divida_fgts")
```

Este vinculo funciona como uma conexão com o banco, portanto poderemos executar consultas nesta tabela que foi referenciada.

Vamos verificar os campos da tabela

```{r DBPlyr head no FGTS}
divida_fgts_rsqlite %>%
  head()
```

Vamos simular a mesma consulta que fizemos anteriormente usando sql, fitraremos os registros do DF, a consulta havia retornado 6610 registros, vamos ver se funciona.

```{r DBplyr consulta fgts no DF}
divida_fgts_rsqlite %>%
  filter(UF_UNIDADE_RESPONSAVEL == 'DF' ) %>%
 count()
```

A quantidade bateu, é possível passar este resultado para uma variável para algum trablhar conforme fariamos no R

```{r DBPlyr retorno dos resultados}
retorno_divida_fgts <- divida_fgts_rsqlite %>%
  filter(UF_UNIDADE_RESPONSAVEL == 'DF' )

as.data.frame(retorno_divida_fgts)
```

Agora na tabela empresa que tem a maior quantidade de registros, vamos trazer alguns registros também.

Primeiro vamos referenciar a tabela com a conexão com o banco

```{r DBPlyr criando vinculo com o banco empresa}
empresa_rsqlite <- tbl(con,"empresa")
```

Não podemos deixar de matar nossa curiosidade se vai funcionar

```{r DBplyr consulta empresas }
empresa_rsqlite %>%
  count()
```

Há!!!! Parece que os numeros são os mesmos, novamente vamos testar nossos computadores, passaremos uma consulta de sumário na tabela empresa

```{r DBPlyr sumario na tabela empresa}
tic()
sumario_empresa_cnae<- empresa_rsqlite %>%
  filter(cnae_fiscal %in% selecao_cnae) %>%
  group_by(cnae_fiscal) %>%
  summarise(n = n())

sumario_empresa_cnae
toc()
```

Sabemos que vocÊs tiveram uma overdose de **sql**, mas para não perder o costume vamos olhar o que está acontecendo na cozinha...

```{r DBPlyr vendo a query}
sumario_empresa_cnae %>% show_query()
```

Parece que temos a boa e velha consulta em sql acontecendo por trás dos panos. 

vamos dar mais uma olhadinha na tabela *movimentacao_caged* que tem mais de 16 milhões de registros.


```{r DBPlyr Criando vinculo com o caged}
#Passamos para uma variável uma funcao do dplyr tbl para conversar com a tabela movimentacao caged no banco
movimentacao_caged_rsqlite <- tbl(con,"movimentacao_caged")
```


Não podemos deixar de contar a quantidade de registros

```{r DBPlyr consulta movimentação_caged}
movimentacao_caged_rsqlite %>%
  count()
```

Coincidentemente os números bateram, agora vamos dar uma olhada nas primeiras linhas

```{r DBPlyr Retorno dos resultados}

movimentacao_caged_rsqlite %>%
  head()

```

Por fim, vamos realizar o mesmo sumário da consulta em da movimentação_caged dos CNAES:

* 6209100 - Suporte Técnico, Manutenção e Outros Serviços em Tecnologia da Informação
* 8230001 - Serviços de Organização de Feiras, Congressos, Exposições e Festas
* 4754701 - Comércio Varejista de Móveis

```{r DBPlyr sumario caged}
simulado_lista_cnae <- c('6209100','8230001','4754701')

movimentacao_caged_rsqlite %>%
  filter(subclasse %in% simulado_lista_cnae) %>%
  group_by(subclasse) %>%
  summarise(soma = sum(saldomovimentacao))
  
```

Pronto, agora você pode escolher a melhor forma de fazer as suas consultas, usando sql diretamente ou o dbplyr.
#Finalização

Analisando nossa jornada:

* baixamos uma boa quantidade de dados;
* extraimos os dados;
* corrigimos alguns detalhes;
* montamos as bases de dados;
* criamos um banco de dados;
* populamos o banco com mais informações;
* realizamos consultas na base sem encher a memória;
* tratamos de diferentes abordagens para as buscas;
* trouxemos os resultados em memória;
* trabalhamos com os resultados da memória.


Agradecemos por acompanhar esta jornada até o final, esperamos que tenham acumulado algum conhecimento.

O nosso muito obrigado!




Criar as seguintes situações

realizar filtro com query direta ´where xxx = xx´
realizar filtro com variável do R
preparar o terreno para uma query em uma grande tabela
  query com subquery

relizar query limitando a quantidade de campos que são retornados
  
Reapresentar o select count 
Fazer consultas com o group by





8230001, 5620102, 7220004, 8230001,

resultado das querys para trabalhar no R





Baixado os dados 

<div/>
  
* Download 
  -  
    - Baixando 1 arquivo  
    - Baixando multiplos arquivos  
      - usando `for` para baixar muitos arquivos 
* Leitura (usar microbench para avaliar os métodos de entrada)  
  -  
    - Lendo 1 arquivo (data.table, vroom)
      -object.size() - ver o tamanho dos itens em memória
    - Lendo multiplos arquivos (cuidado com o tamanho do arquivo!!) 
      - usando `for` para ler multiplos arquivos
  
* A memória encheu... E agora?!
  -  
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf
  
    - Explicando o Banco
    - Visualizando os dados (DB Explorer)
    - Visualizando os dados do banco (pragma do banco, tabela etc)
    - Visalizando os dados no banco (evitar full-scan) `LIMIT X`
    - Fazendo a ingestão dos dados - 1 data frame;
        * Salvando a query em uma nova tabela (para usar como subquery por exemplo);
        * Salvando a consulta em um data.frame (usar em subquery ou realiza análise no R);
    - Fazendo a ingestão dos dados 1 arquivo
    - Criando a tabela de itens a serem filtrados
      - Fazendo a ingestão dos dados (multiplos arquivos);
          - Jogando na memória;
          - Imputando os arquivos `APPEND`;
            - Passando pela memória (oneroso) sei fazer;
            - Fazendo a ingestão direta, estou no caminho;
  - Me deram conexão para um banco com muitas linhas e muitos recursos (o_O)(mostrar no Oracle) 
  - Bonus Track - DuckDB (banco otimizado para uso analítico) - Não sei se vou conseguir implantar a tempo.
          