---
title: "Oficina Chicoteando a Máquina para Extrair Dados: como manipular grandes bases em computadores de pouca capacidade"
subtitle: "6º Seminário Internacional sobre Análise de Dados na Administração Pública" 
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Requisitos

* Linguagem R ;
* Linguagem SQL ;
* DB Explorer SQL Lite;
* [R Base](https://cran.r-project.org/bin/windows/base/) atualizado (4.0.2) - *tem se mostrado mais rápido*;

## Bibliotecas

```{r ativando as bibliotecas, warning=FALSE}
#Criar um loop aqui
if(!require('vroom')){
  install.packages("vroom")
  library(vroom)
}

if(!require('RSQLite')){
  install.packages("RSQLite")
  library(RSQLite)
}

if(!require('tidyverse')){
  install.packages("tidyverse")
  library(tidyverse)
}

if(!require('data.table')){
  install.packages("data.table")
  library(data.table)
}
```

* vroom
* RSqlite
* tidyverse
* data.table

## Bases de dados

* [RFB](https://receita.economia.gov.br/orientacao/tributaria/cadastros/cadastro-nacional-de-pessoas-juridicas-cnpj/dados-publicos-cnpj)
  - [Dicionário de dados](http://200.152.38.155/CNPJ/LAYOUT_DADOS_ABERTOS_CNPJ.pdf)
  
* [SEPT - Secretaia Especial de Previdência e Trabalho](http://pdet.mte.gov.br/microdados-rais-e-caged) - **Novo Caged 2020**
  - [Dicionário de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx)
  
* [PGFN](http://dadosabertos.pgfn.gov.br/)

# Atenção! Caso tenha atualizado o R para a versão 4:


* Atualize o R Studio  

* Atualize o RTools para a versão 4 - Rtools4  
  - Adicione a variável de ambiente no R para que o Rtools4 funcione: `writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")`  

* Em alguns pacotes que apresentarem erro de depêndencia, tente usar o  `install.packages("nome do pacote", dependencies = TRUE)`

* As consultas que vamos realizar podem demorar algum tempo, manteremos vocÊs informados.



## Para quem só sabe usar matelo, **TODO PROBLEMA É UM PREGO** - *Abraham Maslow*
<div style="text-align: justify">

Por isso vamos usar R + SQL

Concordando conosco ...rs , em seu livro [Efficient R Programming](https://csgillespie.github.io/efficientR/) Colin Gillespie e Robin Lovelace falam sobre o uso de bancos de dados com o R no [capitulo 6](https://csgillespie.github.io/efficientR/data-carpentry.html) - *6.6 Working with databases*:

"Instead of loading all the data into RAM, as R does, databases query data from the hard-disk. This can allow a subset of a very large dataset to be defined and read into R quickly, without having to load it first."

Veja o quanto pode ser interessante este uso.

Dizem que 80% do tempo do cientista de dados é destinado para ajustar as bases, mas além de missing values, outliers e outros problemas, temos o aumento do volume de dados  disponíveis. 
Alguns analistas que trabalham em R já devem ter lotado a memória com uma base mais robusta. 
E aí, até conseguir provisionar alguma instância, pedir mais memória, ou conseguir autorização para usar um servidor de maior capacidade, é possível usar recursos alternativos para progredir com o trabalho. 

<div/>

# Chegou a hora .... Mãos na massa!

**Plano de ataque**

Nosso objetivo será fazer uma consulta realizando um sumário de 3 bases de dados diferentes que não caberiam na memória.
 
Vamos baixar dos dados, verificar alguns detalhes que podem aparecer e corrigir pequenos problemas . Depois disso vamos criar um banco de dados para receber as bases de dados. Faremos a ingestão destes dados no banco: diretamente do data.frame . De posse de um banco populado com os dados necessários realizaremos as consultas.

## Baixando os dados

<div style="text-align: justify">

A primeira situação que nos deparamos é baixar as bases, são diferentes fontes **(http, ftp)**, disponibilizadas de diferentes formas **(csv, zip, gz)**, além da quantidade que pode variar, 1 arquivo por período, 1 arquivo com todos os dados.

Será disponibilizado um link alternativo de download das bases, uma quantidade de conexões simultânea solicitando o arquivo nos servidores  pode ser interpretado como um comportamento anormal ou ataque. Desculpe!

### Baixando 1 arquivo 

**PGFN**  
As base de dados da [PGFN](http://dadosabertos.pgfn.gov.br/) contem os dados dos devedores do FGTS, previdênciários e não previdênciários, são arquivos compactados pelo tema (FGTS, Previdênciário ...) contendo arquivos **csv** para cada estado com competência mensal.

Será criada uma pasta para armazenar estes downloads para facilitar a posteior leitura

```{r Download dados PGFN, eval=FALSE}
## Dados de dividas
#diretorio para amazenamento
dir.create("./divida")

#PGFN Divida Ativa Geral - site está fora do ar e este arquivo é maior base - talvez trabalharemos somente com o FGTS
#download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_Nao_Previdenciario.zip", "./divida/Dados_abertos_Nao_Previdenciario.zip", method = "wininet")

#Divida FGTS - conforme documentação testar o método WB (Windows)
download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_FGTS.zip", "./divida/Dados_abertos_FGTS.zip", method = "auto")

#Divida Previdenciaria

#download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_Previdenciario.zip", "./divida/Dados_abertos_Previdenciario.zip", method = "auto")

#tamanho da base 1Gb

```

Foram baixados `r ` Mb de dados compactados


### Baixando multiplos arquivos

**Novo Caged**

Os microdados do [CAGED](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/) são fornecidos ao público por um ftp aberto em que os dados são dispostos em pastas por mês e cada pasta contem arquivos em formato **.7z** :( de todo o ano calendário. Faremos a leitura de todo o diretório e o download dos arquivos em pasta específica do tema.

```{r Download dados CAGED, eval=FALSE}
#Dados do CAGED
#diretorio para armazenamento
dir.create("./caged")

#FTP de acesso para os microdados
#atencao o mês foi especificiado manualmente, até a confeção deste, os dados do mês de agosto não foram disponibilizados.
url = "ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/Julho/"
filenames = getURL(url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
filenames <- strsplit(filenames, "\r\n")
filenames = unlist(filenames)

filenames

#laço baixando todos os arquivos dentro da pasta 
for (filename in filenames) {
  download.file(paste(url, filename, sep = ""), paste(getwd(), "/caged/", filename,
                                                      sep = ""), method = "curl")
}

#cuidado com o metodo ao baixar do FTP o metodo pode corromper o arquivo  
```

Foram baixados `r ` Mb de dados compactados.

### Atenção - Base CNPJ

**CNPJ´s**

Conforme indicamos no início da oficina a base de CNPJ foi disponibilizada já em formato SQLite para download. 

Respeitando o licenciamento da base levamos a conhecimento:

A licença do código é [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e dos dados convertidos [Creative Commons Attribution ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Fonte original e quem tratou os dados:Receita Federal do Brasil, dados tratados por Álvaro Justen/[Brasil.IO](https://brasil.io/).

Das tabelas dispobilizada mantivemos somente a que continha todos os CNPJ´s, de qualquer forma caso tenha curiosidade de ver como foi feita a montagem deste banco em sqlite  teremos um chunk mais abaixo **que está inativo** - ` eval=FALSE` que realiza o processo de ingestão dos dados caso tenha mais um tempinho.


[Link para download do sqlite com os dados dos CNPJ´s - 3.5Gb - ZIP]() - publicar um drive com este arquivo


Terminamos assim o dowload dos aquivos, baixamos diferentes bases de dados em tamanho e características, agora estes arquivos serão trabalhados para que seja possível obter algum resultado.

## Leitura dos dados

Com os dados baixados, começa a jornada para de lê-los e ajustá-los ou vice-versa . 

### Lendo 1 arquivo

**PGFN**
O arquivo com os dados da PGFN com as dívidas do FGTS será lido e passado para um data.frame, usaremos o pacote *vroom* 

```{r Leitura dados PGFN, eval= FALSE }
#primeira pegadinha, o VROOM pode ser muito rápido, porem se você tiver multiplos arquivos dentro do zip ele precisa de uma ajudinha
dados_fgts_com_vroom <- vroom("./divida/Dados_abertos_FGTS.zip") # aqui temos 1170 Observacoes 

dados_fgts_com_vroom<-dados_fgts_com_vroom[,-4] #RETIRANDO O NOME DA EMPRESA

head(dados_fgts_com_vroom)
nrow(dados_fgts_com_vroom)
```

Conforme apresentado o arquivo compactado que estamos tratando possui um *.csv* para cada estado e a quantidade de **1170** observações não parece condizer com a quantidade de empresas inscritas em divida com no FGTS, bem como, os caracteres quebrados podem dificultar as manipulações dos dados. A base foi disponiblizada com caracteres que precisaremos informar para o pacote para que sejam apresentados. 
Outro detalhe é que precisaremos criar uma função que lerá todos os arquivos dentro do *.zip* e passe cada 1 para o *vroom* e assim passá-los para um data.frame.

```{r Leitura dados FGTS, eval= FALSE}
##Instrução para ler os caractereres corratamete
locale_padrao_pt <- locale("pt", encoding = "latin1")


#Criando um wrapper para ler todos os dados dentro do arquivo para o VROOM ler de forma correta
ler_arquivos_dentro_zip <- function(file, ...) {
  nome_dos_arquivos <- unzip(file, list = TRUE)$Name
  vroom(purrr::map(nome_dos_arquivos, ~ unz(file, .x)), locale = locale_padrao_pt,   ...)
}

#passando para o wrapper o caminho do arquivo para leitura de todos os dados
tic()
leitura_direta_vroom <- ler_arquivos_dentro_zip("./divida/Dados_abertos_FGTS.zip")
toc()

```

**LEMBRA DE EXCLUIR O CAMPO NOME_DEVEDOR E TROCAR PARA DATA.TABLE**

Agora que cada arquivo foi passado ao *vroom* e depositado em um data.frame contabilizou-se mais de 440 mil observações, bem diferente do primeiro valor contabilizado
```{r Verificando o dados da base FGTS}
leitura_direta_vroom <- leitura_direta_vroom[,-4] #RETIRANDO O NOME DA EMPRESA
head(leitura_direta_vroom)
nrow(leitura_direta_vroom)
```

Por fim nestes dados o campo **CPF_CNPJ** está com a máscara, e para a realização das consultas iremos retirá-la
```{r Retirar máscara do CNPJ}
#funcao do pacote tm que remove pontuacao
removePunctuation(leitura_direta_vroom$CPF_CNPJ) -> leitura_direta_vroom$CPF_CNPJ
```


### Lendo multiplos arquivos

Ao se trabalhar com os  microdados do CAGED temos alguns percalços. Ao verificar o [dicionario de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx) os nomes das colunas foram escritos com acentuação e os arquivos são compactados em formato *.7z*. Para a primeira situação realizaremos o tratamento das colunas, quanto ao formato recomendamos ,quanto a compactação dos arquivos, para o andamento desta oficina, que vá na pasta `caged` do projeto e extraia os arquivos :( .

```{r Leitura dados CAGED, eval = FALSE}
tic()
#criando uma lista com arquivos txt no diretório
caged_lista_arquivos <- fs::dir_ls("./caged/", glob = "*txt")
#realizando a leitura dos dados  forçando a coluna saldo movimentação com inteiro e demais colunas como caracater
caged_movimentacao <- vroom(caged_lista_arquivos, col_types = list(.default = "c", saldomovimentação = "i" ))
toc()
```
Verificando o nome das colunas :

```{r Verificando o nome das colunas caged}
names(caged_movimentacao)
```

Realizamos a leitura de mais de 16 milhões de linhas com 24 colunas e a carga de mais dados pode começar a ficar difícil. Dando continuidade trataremos os nomes das colunas retirando os espaços, acentuação para trabalhar com estes dados.


```{r Tratando nomes das colunas CAGED, eval= FALSE}
#Tratando os nomes
caged_movimentacao %>%
  names() %>% #pega os nomes das colunas
  stri_trans_general("Latin-ASCII")  -> names(caged_movimentacao)  # substitui caracteres latin para asci
 
```

Com os nomes padronizados estamos com mais uma base pronta .


```{r Nomes das colunas do CAGED }
#Verificando o numero de linhas
nrow(caged_movimentacao)
#verificando a nomenclatura das colunas
names(caged_movimentacao)
```

#### Atenção - Base de CNPJ - Para fazer fora da oficina

Aqui temos um trabalho para você realizar **depois da oficina** que é fazer a carga dos dados de todas as empresas no banco sqlite em sua máquiba, lembre-se recomendamos não realizá-lo durante a oficina pois pode durar mais de 50 minutos. Porem, para fins de aprendizado, disponbilizados o código de como realizar a ingestão de um grande arquivo sem ter que descompactá-lo e passar diretamente para uma base de dados.

**O chunk abaixo está desativado**
```{r Criando o banco cnpj, eval= FALSE}
#Este chunk está desativado. 
#Ative-o somente caso tenha tempo para aguardar o download da base e sua ingestão.
#todos os cnpjs 
dir.create("./cnpj")

#maior base de dados 2.5Gb o tempo pode variar bem confome o a conexão
download.file("https://data.brasil.io/dataset/socios-brasil/empresa.csv.gz", "./cnpj/empresa.csv.gz", method = "auto")



#criando conexão com o banco
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")

#esta funcao e necessaria para alterar o tratamento de entrada dos dados das colunas.
#e criada uma tabela de streamming que lê o arquivo e repassar para o arquivo sqlite
tabela_temp_streaming_cnpj <- function() {
  streamable_table(
    function(file, ...) readr::read_csv(file,col_types = list(.default = "c"),  ...),
    function(x, path, omit_header)
      readr::write_tsv(x = x, path = path, omit_header = omit_header),
    "tsv")
}

tic()
#unark pega arquivo gz passa para uma tabela de streaming gera aquivos de 100000 linhas  e passa para o banco sqlite
unark("./cnpj/empresa.csv.gz", con, lines=100000, streamable_table = tabela_temp_streaming_cnpj() )
toc()

#lembrar sempre de disconectar do banco
dbDisconnect(con)
```


# Com todas estas bases a memória pode encher, e agora?

Os dados vão aparecendo, a necessidade de agregar mais variáveis ou pegar algum atributo de outra base e chega uma hora que não se tem mais memória para o R trabalhar ou carregar mais dados, somente a base de cnpj´s em um único arquivo compactado tem 2.4Gb ao descompatá-lo são mais de 8Gb que você deveria carregar na memória.

# SQLITE
PATRICIA  
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf

Chegamos no ápice da oficina, agora que foram visualizados dados que já estavam em um banco, criaremos um conforme as nossas necessidades.

## Criando o banco ou abrindo um existente

Para criar um novo arquivo que receberá o banco ou ler um existente a sintaxe é a mesma e o código se encarrega de entender se o arquivo já existe ou não.

```{r Conexao com o banco RSQLite}
#Pasando para uma variável as instruções de conexao e driver - atencao para o nome do arquivo
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")
```

Ao executar o chunk acima o R está conectado com este arquivo e usando a API do DBI pode passar instruções  para que o RSQLite processe, inclusive em SQL !

Como temos um banco populado, alem de verificar com o dbExplorer, podemos verificar pelo R. Como a conexão acima já foi realizada ela está ativa e você pode passar a instrução diretamente, porem, ao final de seu código lembre sempre de fechar a conexão com o banco.


```{r Verificando as tabelas do banco}
#listando as tabelas do banco
dbListTables(con)
```

Assim como fazemos no R podemos verificar quais campos esta tabela possui.

```{r Verificando os campos da tabela empresa}
dbListFields(con, "empresa")
```

Vamos olhar os dados desta tabela 

```{r Verificando os dados da tabela empresa}
primeira_query_empresa <- dbGetQuery(con, "select * from empresa limit 10") #LIMIT é importante !!!
primeira_query_empresa
```

Veja que abaixo do nome do campo o tipo dele é apresentado **<chr>**, o cuidado com o tipo de variável é extremamente importante.

E quantos registros temos nesta tabela

```{r Contando registros da tabela empresa}
tic()
contagem_registros_empresa <- dbGetQuery(con, "select count(*) from empresa")
toc()
contagem_registros_empresa
```
Para varrer toda a tabela e contar todos os registros foi necessário um tempo, Leve em consideração este tempo para o desenho e estratégia das consultas.

Rapidamente, fizemos uma pequena exploratória no banco, temos mais de **44 milhões* de registos com 32 colunas cada. E temos mais duas tabelas em memória que usaremos para interagir com a tabela *empresa* 

## Realizando carga de 1 data.frame

Como conseguimos realizar a leitura dos arquivos da dívida do **FGTS** vamos realizar a ingestão dele no banco.

```{r Carga tabela divida_fgts}
tic()
dbWriteTable(con, #conexão com o banco
             "divida_fgts",#nome da tabela no banco
             leitura_direta_vroom) #nnome do data.frame
toc()
```

Agora com duas tabelas, o cenário começa a ficar um pouco mais robusto, porem, sem carregar na memória, podemos inclusive excluir o data frame com os dados do fgts.

```{r Verificando novamente as tabelas}
# Verificando as tabelas
dbListTables(con)
```
 Consultando os campos da tabela
```{r Verificando os campos da tabela divida_fgts}
#Verificando os campos da tabela
dbListFields(con, "divida_fgts")
```

Consultando os primeiros registros

```{r Verificando os dados da tabela divida_fgts }
dbGetQuery(con, "select * from divida_fgts limit 10")
```
 
Falta uma tabela a `caged_movimentacao` inclusive ela tem uma boa quantidade de registros. Para fixar o conhecimento vamos realizar mais uma carga de dados.

```{r Carga tabela movimentacao_caged}
tic()
dbWriteTable(con,
             "movimentacao_caged",
             caged_movimentacao)
toc()
```

Agora temos um banco com 3 tabelas completas de nossa oficina. Caso precise, por exemplo, disponibilizar os dados para estudo os mesmo já estão agrupados em um único arquivo. Você pode compactá-lo e disponibilizar e passar adiante.

**Verificar no dbExplorer**

```{r Verificando todas as tabelas}
dbListTables(con)
```

## Trabalhando as consultas

As primeiras consultas em sql que foram feitas equivalem a um `head()`, porem, precisamos filtrar os dados, trabalhar com eles. 

8230001, 9319101, 5620102, 7220004, 8230001,


Baixado os dados 

<div/>
  
* Download 
  -  
    - Baixando 1 arquivo  
    - Baixando multiplos arquivos  
      - usando `for` para baixar muitos arquivos 
* Leitura (usar microbench para avaliar os métodos de entrada)  
  -  
    - Lendo 1 arquivo (data.table, vroom)
      -object.size() - ver o tamanho dos itens em memória
    - Lendo multiplos arquivos (cuidado com o tamanho do arquivo!!) 
      - usando `for` para ler multiplos arquivos
  
* A memória encheu... E agora?!
  -  
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf
  
    - Explicando o Banco
    - Visualizando os dados (DB Explorer)
    - Visualizando os dados do banco (pragma do banco, tabela etc)
    - Visalizando os dados no banco (evitar full-scan) `LIMIT X`
    - Fazendo a ingestão dos dados - 1 data frame;
        * Salvando a query em uma nova tabela (para usar como subquery por exemplo);
        * Salvando a consulta em um data.frame (usar em subquery ou realiza análise no R);
    - Fazendo a ingestão dos dados 1 arquivo
    - Criando a tabela de itens a serem filtrados
      - Fazendo a ingestão dos dados (multiplos arquivos);
          - Jogando na memória;
          - Imputando os arquivos `APPEND`;
            - Passando pela memória (oneroso) sei fazer;
            - Fazendo a ingestão direta, estou no caminho;
  - Me deram conexão para um banco com muitas linhas e muitos recursos (o_O)(mostrar no Oracle) 
  - Bonus Track - DuckDB (banco otimizado para uso analítico) - Não sei se vou conseguir implantar a tempo.
          