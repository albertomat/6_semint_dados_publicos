---
title: "Oficina Chicoteando a Máquina para Extrair Dados: como manipular grandes bases em computadores de pouca capacidade"
subtitle: "6º Seminário Internacional sobre Análise de Dados na Administração Pública" 
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Requisitos

* Linguagem R ;
* Linguagem SQL ;
* DB Explorer SQL Lite;
* [R Base](https://cran.r-project.org/bin/windows/base/) atualizado (4.0.2) - *tem se mostrado mais rápido*;

## Bibliotecas

```{r ativando as bibliotecas, warning=FALSE}
#Criar um loop aqui
if(!require('vroom')){
  install.packages("vroom")
  library(vroom)
}

if(!require('RSQLite')){
  install.packages("RSQLite")
  library(RSQLite)
}

if(!require('tidyverse')){
  install.packages("tidyverse")
  library(tidyverse)
}

if(!require('data.table')){
  install.packages("data.table")
  library(data.table)
}
```

* vroom
* RSqlite
* tidyverse
* data.table

## Bases de dados

* [RFB](https://receita.economia.gov.br/orientacao/tributaria/cadastros/cadastro-nacional-de-pessoas-juridicas-cnpj/dados-publicos-cnpj)
  - [Dicionário de dados](http://200.152.38.155/CNPJ/LAYOUT_DADOS_ABERTOS_CNPJ.pdf)
  
* [SEPT - Secreatia especial de Previdência e Trabalho](http://pdet.mte.gov.br/microdados-rais-e-caged) - **Novo Caged 2020**
  - [Dicionário de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx)
  
* [CBO2002](http://www.mtecbo.gov.br/cbosite/pages/downloads.jsf) 

* [PGFN](http://dadosabertos.pgfn.gov.br/)

# Atenção! Caso tenha atualizado o R para a versão 4:


* Atualize o R Studio  

* Atualize o RTools para a versão 4 - Rtools4  
  - Adicione a variável de ambiente no R para que o Rtools4 funcione: `writeLines('PATH="${RTOOLS40_HOME}\\usr\\bin;${PATH}"', con = "~/.Renviron")`  

* Em alguns pacotes que apresentarem erro de depêndencia, tente usar o  `install.packages("nome do pacote", dependencies = TRUE)`

* As consultas podem demorar vários minutos, informaremos o tempo aproximado de cada uma.



## Para quem só sabe usar matelo, **TODO PROBLEMA É UM PREGO** - *Abraham Maslow*
<div style="text-align: justify">

Por isso vamos usar R + SQL

Concordando conosco ...rs , em seu livro [Efficient R Programming](https://csgillespie.github.io/efficientR/) Colin Gillespie e Robin Lovelace falam sobre o uso de bancos de dados com o R no [capitulo 6](https://csgillespie.github.io/efficientR/data-carpentry.html) - *6.6 Working with databases*:

"Instead of loading all the data into RAM, as R does, databases query data from the hard-disk. This can allow a subset of a very large dataset to be defined and read into R quickly, without having to load it first."

Veja o quanto pode ser interessante este uso.

Sabemos que 80% do tempo do cientista de dados é destinado para ajustar as bases, mas além de missing values, outliers e outros problemas, temos o aumento do volume de dados  disponíveis. 
Alguns analistas que trabalham em R já devem ter lotado a memória com uma base mais robusta. 
E aí, até conseguir provisionar alguma instância, pedir mais memória, ou conseguir autorização para usar um servidor de maior capacidade, é possível usar recursos alternativos para progredir com o trabalho. 

<div/>

# Chegou a hora .... Mãos na massa!

**Plano de ataque**

Nosso objetivo será fazer uma consulta realizando um sumário de 3 bases de dados diferentes que não caberiam na memória.
 
Vamos baixar dos dados, verificar alguns detalhes que podem aparecer, corrigir pequenos problemas que aparecem. Depois disso vamos criar um banco de dados para receber as bases de dados. Faremos 2 métodos de ingestão destes dados no banco: diretamente do data.frame e por arquivo compactado. De posse de um banco populado com os dados necessários realizaremos as consultas.

## Baixando os dados

<div style="text-align: justify">

A primeira situação que nos deparamos é baixar as bases, são diferentes fontes **(http, ftp)**, disponibilizadas de diferentes formas **(csv, zip, gz)**, além da quantidade que pode variar, 1 arquivo por período, 1 arquivo com todos os dados.

Será disponibilizado um link alternativo de download das bases, uma quantidade de conexões simultânea solicitando o arquivo pode ser interpretado como um comportamento anormal.

### Baixando 1 arquivo 

**PGFN**  
As base de dados da [PGFN](http://dadosabertos.pgfn.gov.br/) contem os dados dos devedores do FGTS, previdênciários e não previdênciários, são arquivos compactados pelo tema (FGTS, Previdênciário ...) contendo arquivos **csv** para cada estado com competência mensal.

Será criada uma pasta para armazenar estes downloads para facilitar a posteior leitura

```{r Download dados PGFN, eval=FALSE}
## Dados de dividas
#diretorio para amazenamento
dir.create("./divida")

#PGFN Divida Ativa Geral - site está fora do ar e este arquivo é maior base - talvez trabalharemos somente com o FGTS
#download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_Nao_Previdenciario.zip", "./divida/Dados_abertos_Nao_Previdenciario.zip", method = "wininet")

#Divida FGTS - conforme documentação testar o método WB (Windows)
download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_FGTS.zip", "./divida/Dados_abertos_FGTS.zip", method = "auto")

#Divida Previdenciaria

#download.file("http://dadosabertos.pgfn.gov.br/Dados_abertos_Previdenciario.zip", "./divida/Dados_abertos_Previdenciario.zip", method = "auto")

#tamanho da base 1Gb

```

Foram baixados `r ` Mb de dados compactados


### Baixando multiplos arquivos

**Novo Caged**

Os microdados do [CAGED](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/) são fornecidos ao público por um ftp aberto em que os dados são dispostos em pastas por mês e cada pasta contem arquivos em formato **.7z** :( de todo o ano calendário. Faremos a leitura de todo o diretório e o download dos arquivos em pasta específica do tema.

```{r Download dados CAGED, eval=FALSE}
#Dados do CAGED
#diretorio para armazenamento
dir.create("./caged")

#FTP de acesso para os microdados
#atencao o mês foi especificiado manualmente, até a confeção deste, os dados do mês de agosto não foram disponibilizados.
url = "ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/2020/Julho/"
filenames = getURL(url, ftp.use.epsv = FALSE, dirlistonly = TRUE)
filenames <- strsplit(filenames, "\r\n")
filenames = unlist(filenames)

filenames

#laço baixando todos os arquivos dentro da pasta 
for (filename in filenames) {
  download.file(paste(url, filename, sep = ""), paste(getwd(), "/caged/", filename,
                                                      sep = ""), method = "curl")
}

#cuidado com o metodo ao baixar do FTP o metodo pode corromper o arquivo  
```

Foram baixados `r ` Mb de dados compactados.

### Atenção - Base CNPJ

**CNPJ´s**

Conforme indicamos no início da oficina a base de CNPJ foi disponibilizada já em formato SQLite para download. 

Respeitando o licenciamento da base levamos a conhecimento:

A licença do código é [LGPL3](https://www.gnu.org/licenses/lgpl-3.0.en.html) e dos dados convertidos [Creative Commons Attribution ShareAlike](https://creativecommons.org/licenses/by-sa/4.0/). Fonte original e quem tratou os dados:Receita Federal do Brasil, dados tratados por Álvaro Justen/[Brasil.IO](https://brasil.io/).

Das tabelas dispobilizada mantivemos somente a que continha todos os CNPJ´s, de qualquer forma caso tenha curiosidade de ver como foi feita a montagem deste banco em sqlite  teremos um chunk mais abaixo **que está inativo** - ` eval=FALSE` que realiza o processo de ingestão dos dados caso tenha mais um tempinho.


[Link para download do sqlite com os dados dos CNPJ´s - 3.5Gb - ZIP]() - publicar um drive com este arquivo

```{r Download dados CNPJs, eval=FALSE}
#Este chunk está desativado, sugerimos baixar a base do link acima e salvar na raiz do projeto. 
#Ative-o somente caso tenha tempo para aguardar o download da base e sua ingestão.
#todos os cnpjs e cnaes e cnaes Secundários
dir.create("./cnpj")

#maior base de dados 2.5Gb o tempo pode variar bem confome o a conexão
download.file("https://data.brasil.io/dataset/socios-brasil/empresa.csv.gz", "./cnpj/empresa.csv.gz", method = "auto")
```

Terminamos assim a primeira chicotada, baixamos diferentes bases de dados em tamanho e características, agora estes arquivos serão trabalhados para que seja possível obter algum resultado.

## Leitura dos dados

Com os dados baixados, começa a jornada para de lê-los e ajustá-los . 

### Lendo 1 arquivo

**PGFN**
O arquivo com os dados da PGFN com as dívidas do FGTS será lido e passado para um data.frame, usaremos o pacote *vroom* 

```{r Leitura dados PGFN, eval= FALSE }
#primeira pegadinha, o VROOM pode ser muito rápido, porem se você tiver multiplos arquivos dentro do zip ele precisa de uma ajudinha
dados_fgts_com_vroom <- vroom("./divida/Dados_abertos_FGTS.zip") # aqui temos 1170 Observacoes 

head(dados_fgts_com_vroom)
nrow(dados_fgts_com_vroom)
```

Conforme apresentado o arquivo compactado que estamos tratando possui um *.csv* para cada estado e a quantidade de **1170** observações não parece condizer com a quantidade de empresas inscritas em divida com no FGTS, bem como, os caracteres quebrados podem dificultar as manipulações dos dados. A base foi disponiblizada com caracteres que precisaremos informar para o pacote para que sejam apresentados. 
Outro detalhe é que precisaremos criar uma função que lerá todos os arquivos dentro do *.zip* e passe cada 1 para o *vroom* e assim passá-los para um data.frame.

```{r Leitura dados FGTS, eval= FALSE}
##Instrução para ler os caractereres corratamete
locale_padrao_pt <- locale("pt", encoding = "latin1")


#Criando um wrapper para ler todos os dados dentro do arquivo para o VROOM ler de forma correta
ler_arquivos_dentro_zip <- function(file, ...) {
  nome_dos_arquivos <- unzip(file, list = TRUE)$Name
  vroom(purrr::map(nome_dos_arquivos, ~ unz(file, .x)), locale = locale_padrao_pt,   ...)
}

#passando para o wrapper o caminho do arquivo para leitura de todos os dados
tic()
leitura_direta_vroom <- ler_arquivos_dentro_zip("./divida/Dados_abertos_FGTS.zip")
toc()

```

Agora que cada arquivo foi passado ao *vroom* e depositado em um data.frame contabilizou-se mais de 440 mil observações, bem diferente do primeiro valor contabilizado
```{r Verificando o dados da base FGTS}
head(leitura_direta_vroom)
nrow(leitura_direta_vroom)
```

Por fim nestes dados o campo **CPF_CNPJ** está com a máscara, e para a realização das consultas iremos retirá-la
```{r Retirar máscara do CNPJ}
#funcao do pacote tm que remove pontuacao
removePunctuation(leitura_direta_vroom$CPF_CNPJ) -> leitura_direta_vroom$CPF_CNPJ
```


### Lendo multiplos arquivos

Ao se trabalhar com os  microdados do CAGED temos alguns percalços. Ao verificar o [dicionario de dados](ftp://ftp.mtps.gov.br/pdet/microdados/NOVO%20CAGED/Movimenta%E7%F5es/Layout%20Novo%20Caged%20Movimenta%E7%E3o.xlsx) os nomes das colunas foram escritos com acentuação e os arquivos são compactados em formato *.7z*. Para a primeira situação realizaremos o tratamento das colunas, quanto ao formato recomendamos ,quanto a compactação dos arquivos, para o andamento desta oficina, que vá na pasta `caged` do projeto e extraia os arquivos :( .

```{r Leitura dados CAGED, eval = FALSE}
tic()
#criando uma lista com arquivos txt no diretório
caged_lista_arquivos <- fs::dir_ls("./caged/", glob = "*txt")
#realizando a leitura dos dados  forçando a coluna saldo movimentação com inteiro e demais colunas como caracater
caged_movimentacao <- vroom(caged_lista_arquivos, col_types = list(.default = "c", saldomovimentação = "i" ))
toc()
```
Verificando o nome das colunas :

```{r Verificando o nome das colunas caged}
names(caged_movimentacao)
```

Realizamos a leitura de mais de 16 milhões de linhas com 24 colunas e a carga de mais dados pode começar a ficar difícil. Dando continuidade trataremos os nomes das colunas retirando os espaços, acentuação para trabalhar com estes dados.


```{r Tratando nomes das colunas CAGED, eval= FALSE}
#Tratando os nomes
caged_movimentacao %>%
  names() %>% #pega os nomes das colunas
  stri_trans_general("Latin-ASCII")  -> names(caged_movimentacao)  # substitui caracteres latin para asci
 
```

Com os nomes padronizados estamos com mais uma base pronta .


```{r Nomes das colunas do CAGED }
#Verificando o numero de linhas
nrow(caged_movimentacao)
#verificando a nomenclatura das colunas
names(caged_movimentacao)
```

#### Atenção - Base de CNPJ - Passo Opcional

Caso vocÊ tenha baixado a base em *sqlite* já com os dados da receita este passo é não deve ser realizado, inclusive recomendamos não realizá-lo durante a oficina pois pode durar mais de 50 minutos. Porem, para fins de aprendizado, disponbilizados o código de como realizar a ingestão de um grande arquivo sem ter que descompactá-lo e passar diretamente para uma base de dados.

**O chunk abaixo está desativado**
```{r Criando o banco cnpj, eval= FALSE}
#criando conexão com o banco
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")

#esta funcao e necessaria para alterar o tratamento de entrada dos dados das colunas.
#e criada uma tabela de streamming que lê o arquivo e repassar para o arquivo sqlite
tabela_temp_streaming_cnpj <- function() {
  streamable_table(
    function(file, ...) readr::read_csv(file,col_types = list(.default = "c"),  ...),
    function(x, path, omit_header)
      readr::write_tsv(x = x, path = path, omit_header = omit_header),
    "tsv")
}

tic()
#unark pega arquivo gz passa para uma tabela de streaming gera aquivos de 100000 linhas  e passa para o banco sqlite
unark("./cnpj/empresa.csv.gz", con, lines=100000, streamable_table = tabela_temp_streaming_cnpj() )
toc()

#lembrar sempre de disconectar do banco
dbDisconnect(con)
```


# Com todas estas bases a memória pode encher, e agora?

Os dados vão aparecendo, as necessidades de agregar mais variáveis vai aumentando e chega uma hora que não se tem mais memória para o R trabalhar ou carregar mais dados, somente a base de cnpj´s em um único arquivo compactado tem 2.4Gb ao descompatá-lo são mais de 8Gb que você deveria carregar na memória.

# SQLITE
PATRICIA
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf

Chegamos no ápice da oficina, dismistificado o assunto, agora que foram visualizados dados que já estavam em um banco, criaremos um conforme as nossas necessidades.

## Criando o banco ou abrindo um existente

Para criar um novo arquivo que receberá o banco ou ler um existente a sintaxe é a mesma e o código se encarrega de entender se o arquivo já existe ou não.

```{r Conexao com o banco RSQLite}
#Pasando para uma variável as instruções de conexao e driver - atencao para o nome do arquivo
con <- dbConnect(RSQLite::SQLite(), "meu_primeiro_banco.db")
```

Ao executar o chunk acima o R está conectado com este arquivo e usando a API do DBI pode passar instruções  para que o RSQLite processe, inclusive em SQL !

Como temos um banco populado, alem de verificar com o dbExplorer, podemos verificar pelo R. Como a conexão acima já foi realizada ela está ativa e você pode passar a instrução diretamente, porem, ao final de seu código lembre sempre de fechar a conexão com o banco.


```{r Verificando as tabelas do banco}
#listando as tabelas do banco
dbListTables(con)
```

Assim como fazemos no R podemos verificar quais campos esta tabela possui.

```{r Verificando os campos da tabela}
dbListFields(con, "empresa")
```

Vamos olhar os dados desta tabela 

```{r Verificando os dados da tabela}
primeira_query_empresa <- dbGetQuery(con, "select * from empresa limit 10") #LIMIT é importante !!!
primeira_query_empresa
```

Veja que abaixo do nome do campo o tipo dele é apresentado **<chr>**, o cuidado com o tipo de variável é extremamente importante.

E quantos registros temos nesta tabela

```{r Contando registros da tabela empresa}
tic()
contagem_registros_empresa <- dbGetQuery(con, "select count(*) from empresa")
toc()
contagem_registros_empresa
```
Para varrer toda a tabela e contar todos os registros foi necessário um tempo, Leve em consideração este tempo para o desenho e estratégia das consultas.

Rapidamente, fizemos uma pequena exploratória no banco, temos mais de **44 milhões* de registos com 32 colunas cada. E temos mais duas tabelas em memória que usaremos para interagir com a tabela *empresa* 

## Realizando carga de 1 data.frame

Como conseguimos realizar a leitura dos arquivos da dívida do **FGTS** vamos realizar a ingestão dele no banco.

```{r Carga tabela divida FGTS}
tic()
dbWriteTable(con, #conexão com o banco
             "divida_fgts",#nome da tabela no banco
             leitura_direta_vroom) #nnome do data.frame
toc()
```

Tem-se agora duas tabelas no 




Baixado os dados 

<div/>
  
* Download 
  -  
    - Baixando 1 arquivo  
    - Baixando multiplos arquivos  
      - usando `for` para baixar muitos arquivos 
* Leitura (usar microbench para avaliar os métodos de entrada)  
  -  
    - Lendo 1 arquivo (data.table, vroom)
      -object.size() - ver o tamanho dos itens em memória
    - Lendo multiplos arquivos (cuidado com o tamanho do arquivo!!) 
      - usando `for` para ler multiplos arquivos
  
* A memória encheu... E agora?!
  -  
  - SQL (a sintaxe) - igual papo doméstico `selecione e conte as coisas do lugar que estão abertas e agrupe por tipo`! :)
  - -> select count(coisas) from lugar where status="abertas" group by = tipo
  - BDI - API para consulta em bancos
  # https://db.rstudio.com/dbi/
  
  - RSQLite - dados parados em disco não incomodam a memória, então, uma ótima solução para quem tem computador de baixa capacidade.
  # https://www.sqlite.org/index.html
  # https://cran.r-project.org/web/packages/RSQLite/RSQLite.pdf
  
    - Explicando o Banco
    - Visualizando os dados (DB Explorer)
    - Visualizando os dados do banco (pragma do banco, tabela etc)
    - Visalizando os dados no banco (evitar full-scan) `LIMIT X`
    - Fazendo a ingestão dos dados - 1 data frame;
        * Salvando a query em uma nova tabela (para usar como subquery por exemplo);
        * Salvando a consulta em um data.frame (usar em subquery ou realiza análise no R);
    - Fazendo a ingestão dos dados 1 arquivo
    - Criando a tabela de itens a serem filtrados
      - Fazendo a ingestão dos dados (multiplos arquivos);
          - Jogando na memória;
          - Imputando os arquivos `APPEND`;
            - Passando pela memória (oneroso) sei fazer;
            - Fazendo a ingestão direta, estou no caminho;
  - Me deram conexão para um banco com muitas linhas e muitos recursos (o_O)(mostrar no Oracle) 
  - Bonus Track - DuckDB (banco otimizado para uso analítico) - Não sei se vou conseguir implantar a tempo.
          